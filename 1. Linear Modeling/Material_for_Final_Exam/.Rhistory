teens <- read.csv("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/4T. Applied Data Mining - SPRING B 2016/Week 3_Clustring/Inclass/snsdata.csv") #change to your own path
dim(teens)
str(teens)
table(teens$gender) #Look a frequency of male and female
table(teens$gender, useNA = "ifany") #Find the frequency of missing gender values
summary(teens$age) #Note the min (3.08), Max (106.90)? Are they teens?
teens$age <- ifelse(teens$age >= 13 & teens$age < 20,
teens$age, NA)
teens$age <- ifelse(teens$age >= 13 & teens$age < 20,
teens$age, NA)
summary(teens$age) #Much better?
teens$female <- ifelse(teens$gender == "F" &
!is.na(teens$gender), 1, 0)
teens$no_gender <- ifelse(is.na(teens$gender), 1, 0)
table(teens$gender, useNA = "ifany") #We have 2,724 cases of unknown gender.
table(teens$female, useNA = "ifany") #We have 7,946 cases of "not female" (meaning it can be male or unknown)
table(teens$no_gender, useNA = "ifany") #We have 2,724 cases of unknown gender. This matches up with our count
mean(teens$age) # Doesn't work b/c of NA
mean(teens$age, na.rm = TRUE) #This tells R to ignore NA. na.rm=TRUE is very common to use.
aggregate(data = teens, age ~ gradyear, mean, na.rm = TRUE)
# Calculating the expected age for each person
#This creates a new variable called ave_age
ave_age <- ave(teens$age, teens$gradyear,
FUN = function(x) mean(x, na.rm = TRUE))
print(ave_age) #To view average age table created above.
teens$age <- ifelse(is.na(teens$age), ave_age, teens$age) #Removes the missing values
#and replaces with average age. This is called imputation. Is this better than removing the missing values?
# check the summary results to ensure missing values are eliminated
summary(teens$age)
set.seed(12345)
interests <- teens[5:40] #Take the 5th through the 40th variables into the model.
?kmeans
teen_clusters_5 <- kmeans(interests, centers=5)
names(teen_clusters_5) #What are the available components of the model you just generated?
teen_clusters_5$size #Show how many people are in each group.
teen_clusters_5$cluster #You can see each row and its assigned cluster (will mean more to us later)
teen_clusters_5$centers #Shows the average of attribute (variable) for each cluster
t(teen_clusters_5$centers) #I tranposed the list to make it easier to read (my preference)
teen_clusters_5$withinss #Within cluster sum of squares (by cluster). A measure of variation.
#Higher value = more dispersed the cluster.
#Lower value = less dispersed the cluster.
#kmeans minimizes within cluster variance (and hence maximizes between cluster variance!)
teen_clusters_5$tot.withinss #Shows the total sum of squares within the clusters.
teen_clusters_5$betweenss #Shows the sum of squares between clusters. How separated the clusters are from each other.
#kmeans maximizes between cluster ss.
teen_clusters_5$totss #Total sum of squares
teen_clusters_5$betweenss/teen_clusters_5$totss #Measure of goodness of fit. How much variation is explained by grouping
#into five clusters as opposed to letting each data point be its own cluster? A relatively larger value indicates more separation
#between the clusters.
teen_clusters_5$withinss/teen_clusters_5$totss #within SS / total_ss #Another measure of goodness of fit. A smaller value
library(cluster) #load this
library(fpc) #load this
plotcluster(interests, teen_clusters_5$cluster, main="k = 5") #creates a viz of the K=5 cluster. Are there distinct groups?
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-200,5), ylim=c(-20,10))
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-200,5), ylim=c(-20,10))
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-20,5), ylim=c(-20,10))
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-20,5), ylim=c(-20,10))
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-10,5), ylim=c(-20,10))
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-10,5), ylim=c(-20,10))
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-4,5), ylim=c(-20,10))
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-4,5), ylim=c(-20,10))
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-4,5), ylim=c(-20,10))
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-2,5), ylim=c(-20,10))
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-2,5), ylim=c(-20,10))
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-2,5), ylim=c(-5,5))
teen_clusters_4 <- kmeans(interests, centers=4) #build a cluster model with 4 cluster categories.
#In K-Means the analyst has a specifiy how many clusters they want. K=?
plotcluster(interests, teen_clusters_4$cluster, main="k=4") #creates a viz of the K=4 cluster. Are there distinct groups?
teen_clusters_3 <- kmeans(interests, centers=3) #build a cluster model with 3 cluster categories.
plotcluster(interests, teen_clusters_3$cluster, main="k=3") #creates a viz of the K=3 cluster.
wss <- (nrow(interests)-1)*sum(apply(interests,2,var))
for (i in 2:10) wss[i] <- sum(kmeans(interests,
centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
library(clusterSim)
teen_clusters_3$size #show how many people are in each group.
Clusters_3<-data.frame(teen_clusters_3$centers) #characteristics of each cluster, in a df to better read
View(Clusters_3)
View(Clusters_3)
Clusters_3<-data.frame(teen_clusters_3$centers) #characteristics of each cluster, in a df to better read
Clusters_3<-data.frame(t(teen_clusters_3$centers)) #transpose to make it easier to read (optional)
Clusters_3[order(-Clusters_3$X1), ] # allows me to view group 1 (X1) by highest variable to lowest.
View(Clusters_3)
View(Clusters_3)
Clusters_3[order(-Clusters_3$X2), ]
teens$cluster <- teen_clusters_3$cluster #adds the cluster number to each recond
#look back at the view of the frame
#This shows the Characteritics of each variable in that cluster
# mean age by cluster
aggregate(data = teens, age ~ cluster, mean)
# proportion of females by cluster
aggregate(data = teens, female ~ cluster, mean)
# mean number of friends by cluster
aggregate(data = teens, friends ~ cluster, mean)
library(AER) #load AER package to get longley dataset # American Economic review
longley
Emp <- longley[,6:7] #We want to look at year and employment
as.character(Emp$year) #year is currently numeric and we are going to transform it to character for clustering purpose.
Emp_dist <- dist(as.matrix(Emp, method="euclidean"))
Emp_dist #View the dissimiliarity matrix
Emp_dist <- dist(as.matrix(Emp, method="euclidean"))
Emp_dist #View the dissimiliarity matrix
#Size of matrix is n*(n-1)/2 = 14*(14-1)/2 = 91 elements.
#Imagine if we have done this for the teens dataset we just looked at!
Emp_hierarchial <- hclust(Emp_dist) #default agglomeration method is complete linkage.
plot(Emp_hierarchial, main = "") #How many clusters should there be?
title("Dendrogam of Employment Figures Using Complete Linkage")
##### Chapter 9: Clustering with k-means -------------------
### Requires packages cluster,clusterSim, fpc,AER, NbClust
### You may need to install them if you don't already have them
## Example: Finding Teen Market Segments ----
## Step 2: Exploring and preparing the data ----
getwd()
teens <- read.csv("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/4T. Applied Data Mining - SPRING B 2016/Week 3_Clustring/Inclass/snsdata.csv") #change to your own path
dim(teens)
str(teens)
##################Clean-up and transform the data#####################################
######## MOST DATA IS NOT CLEAN #############################################
#Clustering does not do well with missing values.
#Have to find and correct them. Very common for a lot of models
# Look at missing data
table(teens$gender) #Look a frequency of male and female
table(teens$gender, useNA = "ifany") #Find the frequency of missing gender values
# Look at missing data for age variable
summary(teens$age) #Note the min (3.08), Max (106.90)? Are they teens?
summary(teens$age) #Note the min (3.08), Max (106.90)? Are they teens?
#Eliminate age outliers. Remember that clustering is very sensitive to outliers.
#Since this is "teens" I defined that as greater than/equal to 13 or lt 20.
#I am chosing to change those ages to NA (You could also delete them)
teens$age <- ifelse(teens$age >= 13 & teens$age < 20,
teens$age, NA)
summary(teens$age) #Much better?
#Recall there were also missing genders
# Reassign missing gender values to "unknown" called na
#creates two new variables. Female and No Gender
teens$female <- ifelse(teens$gender == "F" &
!is.na(teens$gender), 1, 0) #If female & not missing gender value = 1
View(teens)
teens$no_gender <- ifelse(is.na(teens$gender), 1, 0) #Creating a separate variable called "no_gender"
#If gender is unknown then no_gender = 1.
table(teens$gender, useNA = "ifany") #We have 2,724 cases of unknown gender.
table(teens$female, useNA = "ifany") #We have 7,946 cases of "not female" (meaning it can be male or unknown)
table(teens$no_gender, useNA = "ifany") #We have 2,724 cases of unknown gender. This matches up with our count
mean(teens$age) # Doesn't work b/c of NA
mean(teens$age, na.rm = TRUE) #This tells R to ignore NA. na.rm=TRUE is very common to use.
# Review age by cohort
aggregate(data = teens, age ~ gradyear, mean, na.rm = TRUE)
# Calculating the expected age for each person
#This creates a new variable called ave_age
ave_age <- ave(teens$age, teens$gradyear,
FUN = function(x) mean(x, na.rm = TRUE))
print(ave_age) #To view average age table created above.
teens$age <- ifelse(is.na(teens$age), ave_age, teens$age) #Removes the missing values
#and replaces with average age. This is called imputation. Is this better than removing the missing values?
# check the summary results to ensure missing values are eliminated
summary(teens$age)
##########################end of cleaning and transformation###########################
## Step 3: Training a model on the data ----
## Set the seed so that random initial centroid assignment is the same each time
## Value of the seed doesn't matter but using it consistently does.
set.seed(12345)
interests <- teens[5:40] #Take the 5th through the 40th variables into the model.
#Clustering Models via Kmeans
teen_clusters_5 <- kmeans(interests, centers=5)
?kmeans
#Remember that in k-means, the starting centroids are randomly chosen.
#nstart is the number of times the starting points are re-sampled. Think of it this way:
#R does clustering assignment for each data point 25 times and picks the center
#that have the lowest within cluster variation.
#The "best" centroids become the starting point by which kmeans will continue to iterate.
#Typically you can set nstart to between 20 and 25 to find the best overall random start.
#The default algorithm is Hartigan-Wong, which minimizes the within-cluster sum of squares.
#See Morissette & Chartier (2013) paper on Blackboard for explanations of the kmeans algorithms.
#iter.max = maximum number of iterations before stopping (unless convergence is already achieved before max iterations).
names(teen_clusters_5) #What are the available components of the model you just generated?
teen_clusters_5$size #Show how many people are in each group.
teen_clusters_5$cluster #You can see each row and its assigned cluster (will mean more to us later)
teen_clusters_5$centers #Shows the average of attribute (variable) for each cluster
t(teen_clusters_5$centers) #I tranposed the list to make it easier to read (my preference)
teen_clusters_5$withinss #Within cluster sum of squares (by cluster). A measure of variation.
teen_clusters_5$tot.withinss #Shows the total sum of squares within the clusters.
teen_clusters_5$withinss #Within cluster sum of squares (by cluster). A measure of variation.
teen_clusters_5$tot.withinss #Shows the total sum of squares within the clusters.
teen_clusters_5$betweenss #Shows the sum of squares between clusters. How separated the clusters are from each other.
#kmeans maximizes between cluster ss.
teen_clusters_5$totss #Total sum of squares
teen_clusters_5$betweenss/teen_clusters_5$totss #Measure of goodness of fit. How much variation is explained by grouping
#into five clusters as opposed to letting each data point be its own cluster? A relatively larger value indicates more separation
#between the clusters.
teen_clusters_5$withinss/teen_clusters_5$totss #within SS / total_ss #Another measure of goodness of fit. A smaller value
#between the clusters.
teen_clusters_5$withinss/teen_clusters_5$totss #within SS / total_ss #Another measure of goodness of fit. A smaller value
#indicates more cohesion, or tightly packed clusters. ( Smaller value is more cohesive)
#creates a visualization of the clusters. Are there distinct groups?
library(cluster) #load this
library(fpc) #load this
plotcluster(interests, teen_clusters_5$cluster, main="k = 5") #creates a viz of the K=5 cluster. Are there distinct groups?
#If all your data ends up in a corner and hard to read- change the lim for y and x:
#sometime you need to run it first with out the lims and then add them in and run again.
plotcluster(interests, teen_clusters_5$cluster, main="k=5", xlim=c(-2,5), ylim=c(-5,5))
# type ?plotcluster to see the plotcluster arguments.
#Is K=4 better?
teen_clusters_4 <- kmeans(interests, centers=4) #build a cluster model with 4 cluster categories.
#In K-Means the analyst has a specifiy how many clusters they want. K=?
plotcluster(interests, teen_clusters_4$cluster, main="k=4") #creates a viz of the K=4 cluster. Are there distinct groups?
#Is K=3 better?
teen_clusters_3 <- kmeans(interests, centers=3) #build a cluster model with 3 cluster categories.
plotcluster(interests, teen_clusters_3$cluster, main="k=3") #creates a viz of the K=3 cluster.
### 1. Can you use the plots above to pick the appropriate number of clusters?
##2. Compare the cluster plots with between-ss and within-ss
##Remember that kmeans minimizes within-ss and maximizes between-ss.
clusters3<- teen_clusters_3$betweenss/teen_clusters_3$totss
clusters4<- teen_clusters_4$betweenss/teen_clusters_4$totss
clusters5<- teen_clusters_5$betweenss/teen_clusters_5$totss
betweenss.metric <- c(clusters3, clusters4, clusters5)
print(betweenss.metric) #Looking for a ratio that is closer to 1.
#When this ratio is 1, we have a situation where the clusters are tightly packed (cohesion)
#and the space between the clusters are well defined (separation)
#ALWAYS weight the metric against your plots!!
clusters3<- teen_clusters_3$tot.withinss/teen_clusters_3$totss
clusters4<- teen_clusters_4$tot.withinss/teen_clusters_4$totss
clusters5<- teen_clusters_5$tot.withinss/teen_clusters_5$totss
totwithinss.metric <- c(clusters3, clusters4, clusters5)
print(totwithinss.metric) #Looking for a ratio that is closer to 0.
#3. The "Elbow Method" using WithinSS to pick k
#You may need to adjust the iter.max and nstart options to get convergence
#Code is from http://stackoverflow.com/questions/15376075/cluster-analysis-in-r-determine-the-optimal-number-of-clusters
wss <- (nrow(interests)-1)*sum(apply(interests,2,var))
for (i in 2:10) wss[i] <- sum(kmeans(interests,
centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
library(clusterSim)
?index.G1 #read the ../doc/indexG1_details.pdf
a<-index.G1(interests, teen_clusters_3$cluster, centrotypes="centroids") #pseudo F-statistic #look for a relatively large value
b<-index.G1(interests, teen_clusters_4$cluster, centrotypes = "centroids")
c<-index.G1(interests, teen_clusters_5$cluster, centrotypes = "centroids")
pseudoF<-c(a,b,c)
pseudoF
pseudoF
############
#Pick your model using your judgment/business objectives.
#You know your data/business problem best!
##############
teen_clusters_3$size #show how many people are in each group.
#To intrepret the meaning of the clusters. Give each cluster an identity
Clusters_3<-data.frame(teen_clusters_3$centers) #characteristics of each cluster, in a df to better read
Clusters_3<-data.frame(t(teen_clusters_3$centers)) #transpose to make it easier to read (optional)
Clusters_3[order(-Clusters_3$X1), ] # allows me to view group 1 (X1) by highest variable to lowest.
#Easy to intrepret what is important to that group
#do the same for all three clusters. You can also use the sort button for each column.
Clusters_3[order(-Clusters_3$X2), ]
Clusters_3[order(-Clusters_3$X3), ] # Higher the no, more correlation
## Step 5: Improving model performance ----
# apply the cluster IDs to the original data frame
teens$cluster <- teen_clusters_3$cluster #adds the cluster number to each recond
#look back at the view of the frame
#This shows the Characteritics of each variable in that cluster
# mean age by cluster
aggregate(data = teens, age ~ cluster, mean)
# proportion of females by cluster
aggregate(data = teens, female ~ cluster, mean)
# mean number of friends by cluster
aggregate(data = teens, friends ~ cluster, mean)
###############
#And now for something different: AGGLOMMERATIVE (BOTTOM UP) HIERARCHIAL CLUSTERING
###############
library(AER) #load AER package to get longley dataset # American Economic review
longley
Emp <- longley[,6:7] #We want to look at year and employment
as.character(Emp$year) #year is currently numeric and we are going to transform it to character for clustering purpose.
Emp_dist <- dist(as.matrix(Emp, method="euclidean"))
Emp_dist #View the dissimiliarity matrix
#Size of matrix is n*(n-1)/2 = 14*(14-1)/2 = 91 elements.
#Imagine if we have done this for the teens dataset we just looked at!
Emp_hierarchial <- hclust(Emp_dist) #default agglomeration method is complete linkage.
plot(Emp_hierarchial, main = "") #How many clusters should there be?
title("Dendrogam of Employment Figures Using Complete Linkage")
#Default rule of thumb is to prune the tree by the largest difference between two steps (i.e. nodes).
#Default rule of thumb is to prune the tree by the largest difference between two steps (i.e. nodes).
height <- Emp_hierarchial$height #This gives us the height of each node in the dendrogram.
height.2 <- c(0,height[-length(height)]) #This creates a vector with a 0 for the mininum height (bottom of tree) and without the highest height (1 cluster; top of tree)
Emp_hierarchial <- hclust(Emp_dist) #default agglomeration method is complete linkage.
plot(Emp_hierarchial, main = "") #How many clusters should there be?
title("Dendrogam of Employment Figures Using Complete Linkage")
#Default rule of thumb is to prune the tree by the largest difference between two steps (i.e. nodes).
height <- Emp_hierarchial$height #This gives us the height of each node in the dendrogram.
height.2 <- c(0,height[-length(height)]) #This creates a vector with a 0 for the mininum height (bottom of tree) and without the highest height (1 cluster; top of tree)
round(height-height.2,3) #This takes the difference in height at each node.
max(round(height-height.2,3)) #Find the largest increase in distance
which.max(round(height-height.2,3)) #Find the step with the largest increase
#It seems the very last value on the list is the max height. Hence, we should have two clusters. (Remember we removed the option of having 1 cluster.)
##
#Using NbClust package to look at pseudo t^2
library(NbClust)
NbClust(data=Emp, min.nc=2, max.nc=15, method="complete", index="pseudot2")
#Rule: Smallest number of clusters where pseudot2 < criticalvalue
#Why is this so different from the rule of thumb approach above?
#Use your knowledge. Does it make sense to have 10 clusters?
#You will explore more about hierarchical clusters in this week's homework assignment.
#List of all cluster packages: https://cran.r-project.org/web/views/Cluster.html
getwd()
setwd(C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/1M. Linear Modelling - SPRING A 2016/0. Material_for_Final_Exam_Assignment 6)
getwd()
setwd("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/1M. Linear Modelling - SPRING A 2016/0. Material_for_Final_Exam_Assignment 6")
attach(mowers_data)
mowers_data <- read.csv("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/1M. Linear Modelling - SPRING A 2016/0. Material_for_Final_Exam_Assignment 6/mowers_data.csv")
attach(mowers_data)
scale(mowers_data)
NUMERIC_MOWERS_DATA <- mowers_data[-1]
NUMERIC_MOWERS_DATA
describe(NUMERIC_MOWERS_DATA)
library(psych)
library(class)
getwd()
mowers_data <- read.csv("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/1M. Linear Modelling - SPRING A 2016/0. Material_for_Final_Exam_Assignment 6/mowers_data.csv")
attach(mowers_data)
scale(mowers_data)
NUMERIC_MOWERS_DATA <- mowers_data[-1]
NUMERIC_MOWERS_DATA
describe(NUMERIC_MOWERS_DATA)
ZDATA <- scale(NUMERIC_MOWERS_DATA)
describe(ZDATA)
###############K- Mean Cluster###########################
CLUSTERS <- kmeans(ZDATA,2)
CLUSTERS
CLUSTERS$cluster
COMPARE_METHODS<-data.frame(mowers_data$ownership,
CLUSTERS$cluster)
COMPARE_METHODS
EUCIDEAN_DISTS <- dist(ZDATA, method = "euclidean")
EUCIDEAN_DISTS
#############################################
DENDROGRAM <- hclust(EUCIDEAN_DISTS, method = "ward.D")
DENDROGRAM
plot(DENDROGRAM, labels = mowers_data$ownership)
GROUPS <- cutree(DENDROGRAM, k=3)
GROUPS
rect.hclust(DENDROGRAM, k=3, border = "red")
####### hclst --- Hierarchial clustring #####
plot(DENDROGRAM, labels = mowers_data$ownership)
DENDROGRAM <- hclust(EUCIDEAN_DISTS, method = "ward.D")
DENDROGRAM
plot(DENDROGRAM, labels = mowers_data$ownership)
GROUPS <- cutree(DENDROGRAM, k=3)
GROUPS
rect.hclust(DENDROGRAM, k=3, border = "red")
library(psych)
library(stats)
options(digits = 4)
################################################
protein <- read.csv("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/1M. Linear Modelling - SPRING A 2016/0. Material_for_Final_Exam_Assignment 6/protein.csv")
attach(protein)
dim(protein)
names(protein)
str(protein)
protein$Country<-as.character(protein$Country)
summary(protein)
cor(RedMeat, WhiteMeat)
plot(RedMeat, WhiteMeat)
##############################################
PRO_DATA <- scale(protein[,2:3])
PRO_DATA
describe(PRO_DATA)
########################   KMean  ########################
set.seed(1)
CLUSTERS <- kmeans(PRO_DATA,3)
CLUSTERS
CLUSTERS1 <- data.frame(Country, CLUSTERS$cluster)
CLUSTERS1
CLUSTERS1
#############################################
EUCLIDEAN_DISTS <- dist(PRO_DATA, method = "euclidean")
EUCLIDEAN_DISTS
#############################################
DENDROGRAM <- hclust(EUCLIDEAN_DISTS, method = "ward.D")
DENDROGRAM
plot(DENDROGRAM, labels = protein$Country)
GROUPS <- cutree(DENDROGRAM, k=3)
GROUPS
rect.hclust(DENDROGRAM, k=3, border = "red")
####### hclst --- Hierarchial clustring #####
#%%%%%%%%%%%%%% CLUSTER 7###############################################
set.seed(1)
CLUSTERS <- kmeans(PRO_DATA,7)
CLUSTERS
CLUSTERS2 <- data.frame(Country, CLUSTERS$cluster)
CLUSTERS2
#############################################
EUCLIDEAN_DISTS <- dist(PRO_DATA, method = "euclidean")
EUCLIDEAN_DISTS
#############################################
DENDROGRAM <- hclust(EUCLIDEAN_DISTS, method = "ward.D")
DENDROGRAM
plot(DENDROGRAM, labels = protein$Country)
GROUPS <- cutree(DENDROGRAM, k=7)
GROUPS
rect.hclust(DENDROGRAM, k=7, border = "red")
####### hclst --- Hierarchial clustring #####
bakery <- read.csv("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/4T. Applied Data Mining - SPRING B 2016/Week 3_Clustring/Assignment/bakery-binary.csv") #change to your own path
View(bakery)
dim(bakery)
str(bakery)
bakery_clusters_5 <- kmeans(bakery, centers=5)
names(bakery_clusters_5)
bakery_clusters_5$size
bakery_clusters_5$cluster #You can see each row and its assigned cluster (will mean more to us later)
bakery_clusters_5$centers #Shows the average of attribute (variable) for each cluster
t(bakery_clusters_5$centers) #I tranposed the list to make it easier to read (my preference)
bakery_clusters_5$withinss
bakery_clusters_5$tot.withinss #Shows the total sum of squares within the clusters.
bakery_clusters_5$betweenss #Shows the sum of squares between clusters. How separated the clusters are from each other.
bakery_clusters_5$totss #Total sum of squares
bakery_clusters_5$betweenss/teen_clusters_5$totss #Measure of goodness of fit. How much variation is explained by grouping
bakery_clusters_5$betweenss/bakery_clusters_5$totss #Measure of goodness of fit. How much variation is explained by grouping
bakery_clusters_5$withinss
bakery_clusters_5$tot.withinss #Shows the total sum of squares within the clusters.
bakery_clusters_5$betweenss #Shows the sum of squares between clusters. How separated the clusters are from each other.
#kmeans maximizes between cluster ss.
bakery_clusters_5$totss #Total sum of squares
bakery_clusters_5$betweenss/bakery_clusters_5$totss #Measure of goodness of fit. How much variation is explained by grouping
#into five clusters as opposed to letting each data point be its own cluster? A relatively larger value indicates more separation
#between the clusters.
bakery_clusters_5$withinss/bakery_clusters_5$totss #within SS / total_ss #Another measure of goodness of fit. A smaller value
#indicates more cohesion, or tightly packed clusters. ( Smaller value is more cohesive)
#creates a visualization of the clusters. Are there distinct groups?
library(cluster) #load this
library(fpc) #load this
plotcluster(interests, bakery_clusters_5$cluster, main="k = 5") #creates a viz of the K=5 cluster. Are there distinct groups?
bakery <- read.csv("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/4T. Applied Data Mining - SPRING B 2016/Week 3_Clustring/Assignment/bakery-binary.csv") #change to your own path
View(bakery)
dim(bakery)
str(bakery)
bakery_clusters_5 <- kmeans(bakery, centers=5)
names(bakery_clusters_5)
bakery_clusters_5$size
bakery_clusters_5$cluster #You can see each row and its assigned cluster (will mean more to us later)
bakery_clusters_5$centers #Shows the average of attribute (variable) for each cluster
t(bakery_clusters_5$centers) #I tranposed the list to make it easier to read (my preference)
bakery_clusters_5$withinss
bakery_clusters_5$tot.withinss #Shows the total sum of squares within the clusters.
bakery_clusters_5$betweenss #Shows the sum of squares between clusters. How separated the clusters are from each other.
#kmeans maximizes between cluster ss.
bakery_clusters_5$totss #Total sum of squares
bakery_clusters_5$betweenss/bakery_clusters_5$totss #Measure of goodness of fit. How much variation is explained by grouping
#into five clusters as opposed to letting each data point be its own cluster? A relatively larger value indicates more separation
#between the clusters.
bakery_clusters_5$withinss/bakery_clusters_5$totss #within SS / total_ss #Another measure of goodness of fit. A smaller value
#indicates more cohesion, or tightly packed clusters. ( Smaller value is more cohesive)
#creates a visualization of the clusters. Are there distinct groups?
library(cluster) #load this
library(fpc) #load this
plotcluster(interests, bakery_clusters_5$cluster, main="k = 5") #creates a viz of the K=5 cluster. Are there distinct groups?
dim(bakery)
bakery <- read.csv("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/4T. Applied Data Mining - SPRING B 2016/Week 3_Clustring/Assignment/bakery-binary.csv") #change to your own path
dim(bakery)
str(bakery)
set.seed(12345)
Ranbakery <- bakery[1:52] #Take the 5th through the 40th variables into the model.
Ranbakery <- bakery[,1:52] #Take the 5th through the 40th variables into the model.
bakery <- read.csv("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/4T. Applied Data Mining - SPRING B 2016/Week 3_Clustring/Assignment/bakery-binary.csv") #change to your own path
dim(bakery)
str(bakery)
bakery <- read.csv("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/4T. Applied Data Mining - SPRING B 2016/Week 3_Clustring/Assignment/bakery-binary.csv") #change to your own path
dim(bakery)
str(bakery)
set.seed(12345)
Ranbakery <- bakery[,1:52] #Take the 5th through the 40th variables into the model.
str(bakery)
Ranbakery <- bakery[,0:52] #Take the 5th through the 40th variables into the model.
Ranbakery <- bakery[,1:51] #Take the 5th through the 40th variables into the model.
bakery <- read.csv("C:/Users/Avinash Kustagi/Desktop/Desktop/MS BIA/4T. Applied Data Mining - SPRING B 2016/Week 3_Clustring/Assignment/bakery-binary.csv") #change to your own path
dim(bakery)
str(bakery)
set.seed(12345)
Ranbakery <- bakery[,1:51] #Take the 5th through the 40th variables into the model.
bakery_clusters_5 <- kmeans(Ranbakery, centers=5)
names(bakery_clusters_5)
bakery_clusters_5$size
bakery_clusters_5$cluster #You can see each row and its assigned cluster (will mean more to us later)
bakery_clusters_5$centers #Shows the average of attribute (variable) for each cluster
t(bakery_clusters_5$centers) #I tranposed the list to make it easier to read (my preference)
bakery_clusters_5$withinss
bakery_clusters_5$tot.withinss #Shows the total sum of squares within the clusters.
bakery_clusters_5$betweenss #Shows the sum of squares between clusters. How separated the clusters are from each other.
#kmeans maximizes between cluster ss.
bakery_clusters_5$totss #Total sum of squares
bakery_clusters_5$betweenss/bakery_clusters_5$totss #Measure of goodness of fit. How much variation is explained by grouping
#into five clusters as opposed to letting each data point be its own cluster? A relatively larger value indicates more separation
#between the clusters.
bakery_clusters_5$withinss/bakery_clusters_5$totss #within SS / total_ss #Another measure of goodness of fit. A smaller value
#indicates more cohesion, or tightly packed clusters. ( Smaller value is more cohesive)
#creates a visualization of the clusters. Are there distinct groups?
library(cluster) #load this
plotcluster(interests, bakery_clusters_5$cluster, main="k = 5") #creates a viz of the K=5 cluster. Are there distinct groups?
library(fpc) #load this
plotcluster(Ranbakery, bakery_clusters_5$cluster, main="k = 5") #creates a viz of the K=5 cluster. Are there distinct groups?
plotcluster(Ranbakery, bakery_clusters_5$cluster, main="k=5", xlim=c(-2,5), ylim=c(-5,5))
plotcluster(Ranbakery, bakery_clusters_5$cluster, main="k = 5") #creates a viz of the K=5 cluster. Are there distinct groups?
bakery_clusters_6 <- kmeans(Ranbakery, centers=6)
plotcluster(Ranbakery, bakery_clusters_6$cluster, main="k = 6") #creates a viz of the K=5 cluster. Are there distinct groups?
bakery_clusters_7 <- kmeans(Ranbakery, centers=7)
plotcluster(Ranbakery, bakery_clusters_7$cluster, main="k = 7") #creates a viz of the K=5 cluster. Are there distinct groups?
bakery_clusters_8 <- kmeans(Ranbakery, centers=8)
plotcluster(Ranbakery, bakery_clusters_8$cluster, main="k = 8") #creates a viz of the K=5 cluster. Are there distinct groups?
bakery_clusters_7 <- kmeans(Ranbakery, centers=7)
plotcluster(Ranbakery, bakery_clusters_7$cluster, main="k = 7") #creates a viz of the K=5 cluster. Are there distinct groups?
bakery_clusters_7 <- kmeans(Ranbakery, centers=7)
plotcluster(Ranbakery, bakery_clusters_7$cluster, main="k = 7") #creates a viz of the K=5 cluster. Are there distinct groups?
bakery_clusters_8 <- kmeans(Ranbakery, centers=8)
plotcluster(Ranbakery, bakery_clusters_8$cluster, main="k = 8") #creates a viz of the K=5 cluster. Are there distinct groups?
